# EEG-AI-Saliency


## Abstract





There seems to be only 3 sources with similar approach to visual saliency detection. We will explain the method used in each of them:

#### [Visual saliency detection guided by neural signals:](https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://ieeexplore.ieee.org/document/9320159&ved=2ahUKEwig94aDu-yHAxX0TKQEHTeMAf0QFnoECBMQAQ&usg=AOvVaw3ztGyfYLu_r6N34G0MAfA4)

Proposes a novel approach to visual saliency detection guided by brain signals. This method utilizes a two-step process:
* **Learning a Joint Brain-Visual Embedding:** This step involves training two encoders (one for images and one for EEG brain signals) to map them into a shared embedding space. The encoders are trained to maximize the similarity between the embeddings of corresponding images and EEGs. This aims to capture the relationship between what a person is seeing and their brain activity.  
* **Saliency Detection using Compatibility Variations:** Once trained, these encoders analyze how the compatibility between the EEG and image embeddings changes when different regions of the image are suppressed. Regions whose removal causes significant variations in compatibility are considered salient, resulting in a visual saliency map.
* This method combines deep learning's representational power with biological inspiration, aiming to learn visual saliency directly from brain activity.

------------------------------------------------------------
#### [Visual Saliency and Image Reconstruction from EEG Signals via an Effective Geometric Deep Network-Based Generative Adversarial Network:](https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://ieeexplore.ieee.org/document/9320159&ved=2ahUKEwig94aDu-yHAxX0TKQEHTeMAf0QFnoECBMQAQ&usg=AOvVaw3ztGyfYLu_r6N34G0MAfA4)

- input: EEG, Ground truth obtained by eye tracking
- output: image saliency map and full reconstructed image

steps:
1. turn EEG into a graph
2. extract the features of the graph using unsupervised method
3. train the GAN model based on the extracted featues in the last step and ground truth input. and obtain the image saliency map and full reconstructed image

![steps](https://github.com/user-attachments/assets/5aba1387-f8f6-43e5-943f-eed0f37e522f)

Let's take a deep dive into each step:

1. Preprocessing: The first part is obtaining the functional connectivity-based graph representation of the EEG channels. This is done by **Chebyshev graph convolutional layers**.
   - For detailed Explanation visit [here](https://github.com/ab-mahdi/EEG-AI-Salience/edit/main/isual%20saliency%20detection%20via%20learning%20a%20a%20shared%20brain-visual%20representation.md) 


2. Unsupervised Feature Extraction: **Geometric Deep Network (GDN)** is an unsupervised method that takes the graph representation of EEG channels as input, extracts discriminative features from the graph to categorize the EEG signals into different visual patterns.
  - For detailed Explanation visit [here](https://github.com/ab-mahdi/EEG-AI-Salience/edit/main/isual%20saliency%20detection%20via%20learning%20a%20a%20shared%20brain-visual%20representation.md) 

4. Training: **Generative Adversarial Network (GAN) for Saliency and Image Reconstruction:** The features extracted by the GDN are fed into the GAN. This GAN consists of a generator and a discriminator. The generator aims to create a saliency map from the EEG features, while the discriminator tries to distinguish between real saliency maps (derived from eye-tracking data) and those generated by the generator using **saliency metrics**. Through this adversarial process, the generator learns to produce accurate saliency maps from EEG signals.
* The trained GDN-GAN can then be fine-tuned to perform image reconstruction from the EEG signals, going beyond just saliency detection.
  - For detailed Explanation visit [here](https://github.com/ab-mahdi/EEG-AI-Salience/edit/main/isual%20saliency%20detection%20via%20learning%20a%20a%20shared%20brain-visual%20representation.md)

4. Results:
  - For detailed Explanation visit [here](https://github.com/ab-mahdi/EEG-AI-Salience/edit/main/isual%20saliency%20detection%20via%20learning%20a%20a%20shared%20brain-visual%20representation.md) 

-----------------------------------


#### [Salient arithmetic data extraction from brain activity via an improved deep network:](https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://ieeexplore.ieee.org/document/9320159&ved=2ahUKEwig94aDu-yHAxX0TKQEHTeMAf0QFnoECBMQAQ&usg=AOvVaw3ztGyfYLu_r6N34G0MAfA4)

Aim: to consider the Impact of arithmetic concepts on vision-related brain records.

Proposes a Convolutional Neural Network-based Generative Adversarial Network (CNN-GAN) for identifying and extracting arithmetic content (digits 0-9) from visually evoked EEG signals. The method involves:
* **CNN for EEG Classification:**  A CNN is trained on preprocessed EEG signals to classify them into 10 categories, representing digits 0 to 9. 
* **CNN-GAN for Saliency and Image Reconstruction:** The output of the trained CNN is used as input to a GAN, similar to Source 2. This GAN is trained to reconstruct both the salient parts of the MNIST digit images (using SALICON-generated images as ground truth) and the original MNIST digit images themselves. 
* The approach aims to extract not only the category of the visual stimulus but also reconstruct the image itself, particularly the salient regions, directly from EEG data. 

![image](https://github.com/user-attachments/assets/e049051b-4b7b-4c2d-bccd-863e318548b4)


- input: EEG, Ground truth obtained by eye tracking
- output: image saliency map and full reconstructed image

steps:
1. turn EEG into a graph
2. extract the features of the graph using unsupervised method
3. train the GAN model based on the extracted featues in the last step and ground truth input. and obtain the image saliency map and full reconstructed image

![steps](https://github.com/user-attachments/assets/5aba1387-f8f6-43e5-943f-eed0f37e522f)

Let's take a deep dive into each step:

1. Preprocessing: Normalization
   - For detailed Explanation visit [here](https://github.com/ab-mahdi/EEG-Notebook/tree/main) 
No preprocessing?? -> A 14-channel time sample of the EEG dataset is imposed directly as an input signal to the proposed CNN-GAN. The removal of feature vector extraction step results in decreasing the computational load.

2. Unsupervised Feature Extraction: Depth-wise one-dimensional convolution layers to classify the brain signals into 10 different categories according to MNIST image digits. This is done by a **CNN**.
   - The performance of the proposed CNN part is evaluated via the visually provoked 14-channel MindBigData recorded by David Vivancos, corresponding to images of 10 digits. An average accuracy of 95.4% is obtained for the CNN part for classification. 
  - For detailed Explanation visit [here](https://github.com/ab-mahdi/EEG-Notebook/tree/main) 

3. Training: **Generative Adversarial Network (GAN) for Saliency and Image Reconstruction:** The features extracted by the GDN are fed into the GAN. This GAN consists of a generator and a discriminator. The generator aims to create a saliency map from the EEG features, while the discriminator tries to distinguish between real saliency maps (derived from eye-tracking data) and those generated by the generator using **saliency metrics**. Through this adversarial process, the generator learns to produce accurate saliency maps from EEG signals.
* The trained GDN-GAN can then be fine-tuned to perform image reconstruction from the EEG signals, going beyond just saliency detection.
- The performance of the proposed CNN-GAN is evaluated based on saliency metrics of SSIM and CC equal to 92.9% and 97.28%, respectively.
  - For detailed Explanation visit [here](https://github.com/ab-mahdi/EEG-Notebook/tree/main)
-------------------------

4. Results:
  - For detailed Explanation visit [here](https://github.com/ab-mahdi/EEG-Notebook/tree/main) 


## Other Resources

Powerpoint at google slides
continue reading Visual Saliency Detection guided by Neural Signals

Check thise papers:
[Deep Learning Human Mind for Automated Visual Classification](https://arxiv.org/pdf/1609.00344)
[Decoding Brain Representations by Multimodal Learning of Neural Activity and Visual Features](https://arxiv.org/pdf/1810.10974)
[Research on Multimodal Visual Saliency Detection Based on BP Neural Network Algorithm](https://ieeexplore.ieee.org/document/10236637)
[Multimodal contrastive learning for brainâ€“machine fusion: From brain-in-the-loop modeling to brain-out-of-the-loop application](https://www.sciencedirect.com/science/article/abs/pii/S1566253524002252?via%3Dihub)
[research rabbit](https://www.researchrabbit.ai/)
[Brain-Machine Coupled Learning Method for Facial Emotion Recognition](https://ieeexplore.ieee.org/document/10073607)
[Object classification from randomized EEG trials](https://ieeexplore.ieee.org/document/9578178)
Find EEG-Eyetracking Datasets alsocheck kaggle
[An EEG & eye-tracking dataset of ALS patients & healthy people during eye-tracking-based spelling system usage](https://www.nature.com/articles/s41597-024-03501-y)
